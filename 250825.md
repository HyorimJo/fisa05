# 8/25 ML 4일차

## Keywords
과대적합 과소적합

교차검증 K-Fold Stratified K-Fold cross_val_score() GridSearchCV

Confusion Matrix Accuracy Precision Recall F1 Score ROC Curve AUC

***

## Comments
분류와 회귀의 성능지표를 구분해서 알아두자
분류는 정확도 정밀도 재현율 F1-score 등
회귀는 MSE, RMSE 등

***

## 알아둘 내용

## 1. 과대적합, 과소적합

**과대적합(Overfitting)**

- 모델이 학습 데이터에만 과도하게 최적화되어, 실제 예측을 다른 데이터로 수행할 경우에는 예측 성능이 과도하게 떨어지는 것

**과소적합(Underfitting)**

- 모델이 너무 단순하여 학습 데이터를 충분히 학습하지 못함
- 데이터가 불충분하여 학습이 부족함

→ 교차검증을 통해 과대적합/과소적합 방지할 수 있음!

## 2. 교차검증

### 1) K-Fold 교차검증

- 데이터가 적은 경우 별도로 테스트데이터를 확보하면 비효율적
- 가능하면 많은 데이터를 학습에 사용하면서, 성능 평가하는 방법 필요
- K-겹교차검증(k-fold cross-validation)
    - 전체데이터를 k등분하고, 각 등분을 한번씩 테스트데이터로 사용하여, 성능평가를 하고 평균값을 선택하는 방법

```python
# 30개씩 5번 겹치지 않은 인덱스 번호로 훈련/테스트에 데이터를 사용함을 통해
# 우리가 얻게된 성능이 데이터의 잘 섞임(우연성)에 좌우되지 않았음을 신뢰할 수 있게
from sklearn.model_selection import KFold
import numpy as np

# 5개의 폴드 세트로 분할
kfold = KFold(n_splits=5, shuffle=True)
dt_clf = DecisionTreeClassifier(random_state=121)

n_iter = 0

# 평가한 결과치를 적재하기 위해
cv_accuracy = []

# 폴드별 학습용, 검증용 row index를 array 반환
for train_index, test_index in kfold.split(X):
    print(train_index, test_index)
    # 학습용, 검증용 데이터 추출
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    print(y_test)

    # 학습 및 예측
    dt_clf.fit(X_train, y_train)
    pred = dt_clf.predict(X_test)

    n_iter += 1

    # 반복시마다 정확도 측정
    accuracy = np.round(accuracy_score(y_test, pred), 4)
    cv_accuracy.append(accuracy)
    
cv_accuracy
# [np.float64(0.9333),
#  np.float64(0.9333),
#  np.float64(0.8333),
#  np.float64(1.0),
#  np.float64(0.9333)]
```

### 2) Stratified K-Fold

- 불균형한(imbalanced) 분포도를 가진 레이블(결정 class) 데이터 집합을 위한 K 폴드 방식
- 불균형한 분포도란? 레이블 데이터 집합은 특정 레이블 값이 특이하게 많거나 매우 적어서 값의 분포가 한쪽으로 치우치는 것 의미
- K 폴드가 레이블 데이터 집합이 원본 데이터 집합의 레이블 분포를 학습 및 테스트 세트에 제대로 분배하지 못하는 경우의 문제 해결
- 원리 : 원본 데이터의 레이블(target) 분포를 먼저 고려 -> 이 분포와 동일하게 학습과 검증 데이터 세트를 분배

```python
from sklearn.model_selection import StratifiedKFold
# train:test의 비율 뿐 아니라 가지고 있는 y값의 분포도 함께 고려를 해서 교차검증시 사용
# 30개씩 5번 겹치지 않은 인덱스 번호로 훈련/테스트에 데이터를 사용함을 통해
# 우리가 얻게된 성능이 데이터의 잘 섞임(우연성)에 좌우되지 않았음을 신뢰할 수 있게
import numpy as np

# 5개의 폴드 세트로 분할
kfold = StratifiedKFold(n_splits=5, shuffle=True)
dt_clf = DecisionTreeClassifier(random_state=121)

n_iter = 0

# 평가한 결과치를 적재하기 위해
cv_accuracy = []

# 폴드별 학습용, 검증용 row index를 array 반환
for train_index, test_index in kfold.split(X, y): # 분할시 y의 라벨 분포를 고려
    print(train_index, test_index)
    # 학습용, 검증용 데이터 추출
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    print(y_test)

    # 학습 및 예측
    dt_clf.fit(X_train, y_train)
    pred = dt_clf.predict(X_test)

    n_iter += 1

    # 반복시마다 정확도 측정
    accuracy = np.round(accuracy_score(y_test, pred), 4)
    cv_accuracy.append(accuracy)

np.mean(cv_accuracy)
# np.float64(0.9533400000000001)
```

### 3) cross_val_score()

- 교차 검증을 더 쉽게 해주는 API
- 폴드셋 설정 -> for 에서 반복 학습 및 테스트 데이터의 index 추출 -> 반복적으로 학습과 예측 수행 및 성능 평가 : 한번에 처리

```python
from sklearn.model_selection import cross_val_score

# 모델을 만들고
dt_clf = DecisionTreeClassifier(max_depth=5)

# 모델과 모델을 훈련시킬 데이터, fold해서 사용할지를 모두 하이퍼파라미터로 넘겨줍니다.
cross_val_score(dt_clf, X, y, cv=5, scoring='accuracy').mean()
# np.float64(0.9666666666666668)
```

### 4) GridSearchCV

- 교차검증과 최적 하이퍼 파라미터 튜닝을 한 번에
- 격자형식으로 dict 구조의 하이퍼파라미터값 적용

```python
import pandas as pd
# DecisitionTree -> 가지치기를 몇번 할 것인가 max_depth, 몇개 이하의 샘플로 나뉘면 더이상 가지를 치지 않을 것인가 : min_samples_split
from sklearn.model_selection import RandomizedSearchCV

# 일일히 파라미터를 수정해나간다면...
params = {'max_depth' : range(1,6), 'min_samples_split': [2, 3, 4, 5]}
# 1. 클래스 임포트
dt_clf = DecisionTreeClassifier()
# 2. 클래스를 통한 인스턴스 생성 - n_iter로 탐색의 횟수를 제한합니다.
rd_clf = RandomizedSearchCV(dt_clf, param_distributions=params, cv=5, scoring='accuracy', n_iter=20)

# 3. 인스턴스에 해야할 일을 파라미터 또는 fit 이라는 함수로 맡김 - 훈련데이터 사용
# 4. 훈련된 인스턴스에 새로운 데이터(test)를 주면서 평가
rd_clf.fit(X,y)
```

```python
print(rd_clf.best_estimator_)
print(rd_clf.best_index_)
print(rd_clf.best_params_)
print(rd_clf.best_score_)
print(rd_clf.classes_)

# DecisionTreeClassifier(max_depth=3, min_samples_split=4)
# 10
# {'min_samples_split': 4, 'max_depth': 3}
# 0.9733333333333334
# [0 1 2]
```

## 3. 성능 평가

- 모델 성능 평가:
    - 실제값과 모델에 의해 예측된 값을 비교하여 두 값의 차이(오차)를 구하는 것
    - (실제값-예측값) =0 이라면 오차가 없는 것으로, 모델이 값을 100% 맞추는 것!
    - 하지만 예측 값이 실제값과 100% 일치하는 것은 불가능... 어느 정도까지 오차를 허용할지 결정해야 한다.
- 모델 평가의 목적: 과적합(Overfitting)을 방지하고 최적의 모델을 찾기 위해서
- 모델 성능 평가는 종속변수(결과, 답지)가 있어야 맞췄는지 어쩐지 확인할 수 있기 때문에 지도학습에서만 사용할 수 있다.
- 모델링의 목적 또는 목표 변수의 유형에 따라 다른 평가지표를 사용한다.
- Training과 Validation값이 거의 일치해야 좋은 모델이다.
- 만약 Training데이터로는 성능이 좋게 나왔는데, Validation 데이터를 사용했을 때 성능이 확연하게 떨어진다면 모델이 과적합된 상태이다.

```python
from sklearn.metrics import classification_report
print(classification_report(y_true, y_pred))
```

### 1) 오차행렬(Confusion Matrix)

- True Negative: 예측값을 Negative 값 0으로 예측했고, 실제 값도 Negative 값 0
- False Positive: 예측값을 Positive 값 1로 예측했는데, 실제 값은 Negative 값 0. 제 1종 오류
- False Negative: 예측값을 Negative 값 0으로 예측했는데, 실제 값은 Positive 값 1. 제 2종 오류
- True Positive: 예측값을 Positive 값 1로 예측했고, 실제 값도 Positive 값 1

```python
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

y_true = [1, 0, 1, 1, 0, 1]  # y_test
y_pred = [0, 0, 1, 1, 0, 1]  # 모델이 예측한 결과  model.predict(X_test)라고 가정

r = confusion_matrix(y_true, y_pred)
'''
array([[2, 0],       TP     FP
       [1, 3]])      FN     TN
'''

ConfusionMatrixDisplay(r).plot()
```

### 2) 정확도(Accuracy)

정확도 = TP + TN / TP + TN + FP + FN

- 직관적으로 모델의 성능을 나타낼수 있는 평가지표
- '전체 데이터중에, 정확하게 예측한 데이터의 수' - 실제 데이터와 예측 데이터가 얼마나 같은지 평가하는 지표
- scikit-learn에서는 `accuracy_score` 함수를 제공
- 주의: 불균형한 데이터(imbalanced data)의 경우에 정확도는 적합한 평가지표가 아니다.
    
    예시: 광고 노출수와 클릭수는 99 vs 1- 무조건 클릭 아님을 선택하면? 99% 정확도를 가진 분류기가 됨
    

```python
from sklearn.metrics import accuracy_score
accuracy_score(y_true, y_pred) # metrics 안의 함수들은 실제값(정답), 모델의예측값
# 0.8333333333333334
```

### 3) 정밀도(Precision), 재현율(Recall)

- 정밀도 = TP / (FP + TP)
    - '양성으로 판단한 것 중, 진짜 양성의 비율'
- 재현율 = TP / (FN + TP)
    - '진짜 양성인 것들 중에서, 올바르게 양성으로 판단한 비율'
    - 민감도(Sensitivity)라고도 불리우며, 모델의 안정성을 평가하는 지표로 사용됨
- 정확도 = (TN + TP) / (TN + FP + FN + TP)
- 오류율 = (FN + FP) / (TN + FP + FN + TP)

- 정밀도는 실제 Negative(음성)인 데이터를 Positive로 잘못 판단하게 되었을 때 큰 문제가 발생되는 경우 (예시 - 신약의 효과 측정, 날씨 좋다고 해서 운동회 잡았는데 비오는 경우) 더 중요한 지표로 간주되고
- 재현율은 실제 Positive(양성)인 데이터 예측을 Negative로 잘못 판단하게 되면 업무상 큰 영향이 발생하는 경우 더 중요한 지표로 간주된다. (예시 - 암 판정에서의 거짓 양성)

```
# 껌종이를 빈껌으로 만들어놓고
# 10개 껌종이 중에 진짜껌이 8개, 가짜껌이 2개

# 6개를 골랐는데 4개가 진짜검 , 2개가 가짜검

# 내가 고른 껌 중에 진짜껌의 비율  -> 확실한 것만 고르도록
# 정밀도 : 4/6

# 전체 진짜 검 중에 내가 몇개나 골랐는지  -> 되도록 많은 값을 고르도록
# 재현율: 4/8
```

```python
from sklearn.metrics import precision_score
precision_score(y_true, y_pred)
# 1.0

from sklearn.metrics import recall_score
recall_score(y_true, y_pred)
# 0.75
```

```python
from sklearn.metrics import precision_recall_curve

precision, recall, th = precision_recall_curve(y_true, y_pred)

plt.xlabel('threadhold') #임계값
plt.ylabel('score')
plt.plot(th,precision[:len(th)],'red',linestyle = '--',label = 'precision')
plt.plot(th,recall[:len(th)],'blue',label = 'recall')
plt.legend()
plt.show()
```

### 4) F1 Score

- 정밀도와 재현율을 결합한 지표
- 정밀도와 재현율이 어느 한쪽으로 치우치지 않을 때 높은 값을 가짐
- 정확도, 정밀도, 재현율, F1-Score는 모두 0~1 사이의 값을 가지며, 1에 가까워질수록 성능이 좋음을 의미합니다.

```python
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)

(2 * (precision * recall)) / (precision + recall)
# 0.8571428571428571
```

### 5) ROC Curve, AUC

- 수신자 조작 특성 (Receiver operating characteristic)
- 수신자 조작 특성 혹은 반응자 작용특성, 수용자 반응특성은 - 신호탐지이론에서 적중확률 대 오경보확률의 그래프
- ROC 곡선은 FPR(False Positive Rate)이 변할 때 TPR(True Positive Rate)이 어떻게 변하는지 나타내는 곡선
    - TPR(True Positive Rate): TP / (FN + TP), 재현율
    - TNR(True Negative Rate): TN / (FP + TN)
    - FPR(False Positive Rate): FP / (FP + TN), 1 - TNR
- AUC(Area Under Curve) 값은 ROC 곡선 밑에 면적을 구한 값 (1이 가까울수록 좋은 값)
- AUC의 장점
    - AUC는 척도 불변(Scale-Invariant): 절대값이 아닌, 예측이 얼마나 잘 평가되었는지는 측정
    - AUC는 분류 임계값 불변(Classification-Threshold-Invariant): 어떤 분류 임계값이 선택되었는지와 무관하게 모델의 에측 품질을 측정

```python
from sklearn.metrics import roc_auc_score, RocCurveDisplay
roc_auc_score(y_true, y_pred)
# np.float64(0.875)
```